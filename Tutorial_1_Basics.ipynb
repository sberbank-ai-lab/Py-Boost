{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simpliest usage example of py_boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py-boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Optional: set the device to run\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# simple case - just one class is used\n",
    "from py_boost import GradientBoosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dummy regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 1.45 s, total: 3.4 s\n",
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = make_regression(150000, 100, n_targets=10, random_state=42)\n",
    "X_test, y_test = X[:50000], y[:50000]\n",
    "X, y = X[-50000:], y[-50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GBM\n",
    "\n",
    "The only required argument here is the loss funnction. Type of solved task is defined by loss function and input target shape. It could be passed as Loss instance or via string alias:\n",
    "\n",
    "* ***'mse'*** for the regression/multitask regression\n",
    "* ***'msle'*** for the regression/multitask regression\n",
    "* ***'bce'*** for the binary/multilabel classification\n",
    "* ***'crossentropy'*** for the multiclassification\n",
    "\n",
    "\n",
    "Training is simply done by calling .fit metod. Possible arguments:\n",
    "\n",
    "* ***'X'*** \n",
    "* ***'y'*** \n",
    "* ***'sample_weight'*** \n",
    "* ***'eval_sets'*** Validation set is passed as the list of dict with possible keys ['X', 'y', 'sample_weight']. Note: if multiple valid sets are passed, the best model is selected using the last one\n",
    "\n",
    "#### The example below illustrates how to train simple regression task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:25:59] Stdout logging level is INFO.\n",
      "[20:25:59] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[20:25:59] Iter 0; Sample 0, rmse = 173.6750225293403; \n",
      "[20:25:59] Iter 10; Sample 0, rmse = 133.1954926671811; \n",
      "[20:26:00] Iter 20; Sample 0, rmse = 107.86632421368985; \n",
      "[20:26:00] Iter 30; Sample 0, rmse = 90.082420080482; \n",
      "[20:26:00] Iter 40; Sample 0, rmse = 76.44561629937125; \n",
      "[20:26:00] Iter 50; Sample 0, rmse = 65.61091639545339; \n",
      "[20:26:00] Iter 60; Sample 0, rmse = 56.80208140886104; \n",
      "[20:26:00] Iter 70; Sample 0, rmse = 49.577701821537644; \n",
      "[20:26:00] Iter 80; Sample 0, rmse = 43.604032056197624; \n",
      "[20:26:00] Iter 90; Sample 0, rmse = 38.69803479079091; \n",
      "[20:26:00] Iter 99; Sample 0, rmse = 34.992295325052034; \n",
      "CPU times: user 3.98 s, sys: 708 ms, total: 4.69 s\n",
      "Wall time: 3.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.boosting.GradientBoosting at 0x7f245c30a790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = GradientBoosting('mse')\n",
    "\n",
    "model.fit(X, y[:, 0], eval_sets=[{'X': X_test, 'y': y_test[:, 0]},])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for multiregression\n",
    "\n",
    "Each of built-in loss functions has its own default metric, so metric definition is optional. If you need to specify eval metric, you could pass Metric instance or use string alias:\n",
    "\n",
    "#### Default metrics:\n",
    "\n",
    "* ***'rmse'*** is the default for the ***'mse'*** loss\n",
    "* ***'rmsle'*** is the default for the  ***'msle'*** loss\n",
    "* ***'bce'*** is the default for the ***'bce'*** loss\n",
    "* ***'crossentropy'*** is the default for the ***'crossentropy'*** loss\n",
    "\n",
    "#### Non default metrics:\n",
    "\n",
    "* ***'r2'*** for the regression/multitask regression\n",
    "* ***'auc'*** for the binary classification\n",
    "* ***'accuracy'*** for any classification task\n",
    "* ***'precision'*** for any classification task\n",
    "* ***'recall'*** for any classification task\n",
    "* ***'f1'*** for any classification task\n",
    "\n",
    "\n",
    "Also you can specify other common GBDT hyperparameters as shown below\n",
    "\n",
    "#### The example below demonstrates the example of training model for multioutput regression task, no extra definition needed to switch the task, just pass multidimensional target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:26:01] Stdout logging level is INFO.\n",
      "[20:26:01] GDBT train starts. Max iter 1000, early stopping rounds 200\n",
      "[20:26:01] Iter 0; Sample 0, R2_score = 0.0083971580680002; \n",
      "[20:26:03] Iter 100; Sample 0, R2_score = 0.5170809873680927; \n",
      "[20:26:04] Iter 200; Sample 0, R2_score = 0.7244484108548253; \n",
      "[20:26:06] Iter 300; Sample 0, R2_score = 0.8327945671515238; \n",
      "[20:26:07] Iter 400; Sample 0, R2_score = 0.8949818250346425; \n",
      "[20:26:09] Iter 500; Sample 0, R2_score = 0.9320841598288354; \n",
      "[20:26:11] Iter 600; Sample 0, R2_score = 0.9546882993697207; \n",
      "[20:26:12] Iter 700; Sample 0, R2_score = 0.9687345404426747; \n",
      "[20:26:14] Iter 800; Sample 0, R2_score = 0.9776046975136594; \n",
      "[20:26:16] Iter 900; Sample 0, R2_score = 0.983284793877143; \n",
      "[20:26:17] Iter 999; Sample 0, R2_score = 0.9869747857432605; \n",
      "CPU times: user 16.8 s, sys: 1.66 s, total: 18.5 s\n",
      "Wall time: 17.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.boosting.GradientBoosting at 0x7f227a4c9ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = GradientBoosting('mse', 'r2_score',\n",
    "                         ntrees=1000, lr=.01, verbose=100, es=200, lambda_l2=1,\n",
    "                         subsample=.8, colsample=.8, min_data_in_leaf=10, min_gain_to_split=0, \n",
    "                         max_bin=256, max_depth=6)\n",
    "\n",
    "model.fit(X, y, eval_sets=[{'X': X_test, 'y': y_test},])ли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "#### Prediction could be done via calling .predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 795 ms, sys: 491 ms, total: 1.29 s\n",
      "Wall time: 1.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-245.00179 , -154.63591 , -283.4694  , ..., -149.77898 ,\n",
       "        -225.69731 , -242.11638 ],\n",
       "       [-109.20388 , -106.41216 ,  -47.461292, ..., -120.21821 ,\n",
       "        -112.02216 ,  -11.135624],\n",
       "       [ -41.005196,  -66.21285 ,  138.89832 , ...,   11.006489,\n",
       "         -31.018751, -215.91324 ],\n",
       "       ...,\n",
       "       [ -74.87248 ,  138.37398 ,   81.35473 , ...,  228.11902 ,\n",
       "          39.559467,   20.036104],\n",
       "       [ -13.227322,  128.68654 ,  232.82118 , ...,  136.53038 ,\n",
       "         160.83208 ,  199.1988  ],\n",
       "       [ -14.531125,   39.43056 ,  171.69994 , ...,   99.01615 ,\n",
       "          26.81745 ,    7.780988]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for certan iterations could be done via calling .predict_staged method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 ms, sys: 220 ms, total: 466 ms\n",
      "Wall time: 465 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 50000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict_staged(X_test, iterations=[100, 300, 500])\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree leaves indicies prediction for certan iterations could be done via calling .predict_leaves method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 ms, sys: 7.57 ms, total: 18.8 ms\n",
      "Wall time: 17.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 50000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict_leaves(X_test, iterations=[100, 300, 500])\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 36, 19],\n",
       "       [35, 48, 18],\n",
       "       [31, 56, 21],\n",
       "       ...,\n",
       "       [59, 56, 26],\n",
       "       [30, 57, 20],\n",
       "       [53, 56, 20]], dtype=uint32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.,   51.,   37.,   43.,   54.,   51., 5674.,   52.,   62.,\n",
       "         55.,   40.,   48.,   45.,   33.,   57., 5945., 5510.,   40.,\n",
       "         37., 5460.,   29.,   35.,   52.,   62.,   34.,   41.,   48.,\n",
       "         49.,   30.,   37.,   48.,   46.,   56.,   43.,   37.,   43.,\n",
       "       6009.,   43.,   39.,   40.,   64.,   52.,   44.,   49.,   62.,\n",
       "         38.,   39.,   46.,   41.,   50.,   52.,   58., 5848.,   47.,\n",
       "         38.,   60.,   55.,   36.,   35.,   50.,   34.,   48.,   41.,\n",
       "         36.,   38.,   41.,   38.,   43.,   44.,   65.,   45.,   29.,\n",
       "         39.,   64.,   44.,   59.,   65.,   37.,   44.,   40.,   51.,\n",
       "         38.,   51.,   49.,   59.,   38., 5605., 3504.,   55., 5843.,\n",
       "         53., 6135.,   41.,   50.,   53.,   39.,   47.,   42.,   56.,\n",
       "         65.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained model could be saved as pickle for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-245.00179 , -154.63591 , -283.4694  , ..., -149.77898 ,\n",
       "        -225.69731 , -242.11638 ],\n",
       "       [-109.20388 , -106.41216 ,  -47.461292, ..., -120.21821 ,\n",
       "        -112.02216 ,  -11.135624],\n",
       "       [ -41.005196,  -66.21285 ,  138.89832 , ...,   11.006489,\n",
       "         -31.018751, -215.91324 ],\n",
       "       ...,\n",
       "       [ -74.87248 ,  138.37398 ,   81.35473 , ...,  228.11902 ,\n",
       "          39.559467,   20.036104],\n",
       "       [ -13.227322,  128.68654 ,  232.82118 , ...,  136.53038 ,\n",
       "         160.83208 ,  199.1988  ],\n",
       "       [ -14.531125,   39.43056 ,  171.69994 , ...,   99.01615 ,\n",
       "          26.81745 ,    7.780988]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'temp_model.pkl')\n",
    "\n",
    "new_model = joblib.load('temp_model.pkl')\n",
    "new_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda_py38",
   "language": "python",
   "name": "anaconda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
